{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steps\n",
    "- Data collection\n",
    "- Tokenization\n",
    "- Stopword Removal\n",
    "- Stemming/Lemmatization\n",
    "- Build a common vocab\n",
    "- Vectorize the sentence\n",
    "- Machine Learning Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories=['adventure'])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ['He', 'certainly', \"didn't\", 'want', 'a', 'wife', 'who', 'was', 'fickle', 'as', 'Ann', '.'], ['If', 'he', 'had', 'married', 'her', ',', \"he'd\", 'have', 'been', 'asking', 'for', 'trouble', '.'], ['But', 'all', 'of', 'this', 'was', 'rationalization', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan Morgan told himself he would forget Ann Turner .He was well rid of her .He certainly didn't want a wife who was fickle as Ann .If he had married her , he'd have been asking for trouble .But all of this was rationalization .\n"
     ]
    }
   ],
   "source": [
    "document = \"\"\n",
    "for i in range(5):\n",
    "    document += ' '.join(data[i])\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Dan Morgan told himself he would forget Ann Turner. He was well rid of her. He certainly didn't want a wife who was fickle as Ann. If he had married her , he'd have been asking for trouble. But all of this was rationalization .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan Morgan told himself he would forget Ann Turner.', 'He was well rid of her.', \"He certainly didn't want a wife who was fickle as Ann.\", \"If he had married her , he'd have been asking for trouble.\", 'But all of this was rationalization .']\n"
     ]
    }
   ],
   "source": [
    "corpus = sent_tokenize(test)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Email', 'me', 'all', 'the', 'class', 'learning', 'assignments', 'of', 'the', 'class', 'at', 'xyz', 'gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "text = \"Email me all the10 class learning assignments of the class 1,3,4 at xyz@gmail.com\"\n",
    "reg_exp = \"[a-zA-Z]+\"\n",
    "all_word_except = \"[\\b (?:the) \\b]\"\n",
    "tokenizer = RegexpTokenizer(reg_exp)\n",
    "text = tokenizer.tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sent,stopwords):\n",
    "    useful_words = [w for w in sent if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_stopwords(text,stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Email', 'class', 'learning', 'assignments', 'class', 'xyz', 'gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer,PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gener'"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"generous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan Morgan told himself he would forget Ann Turner.', 'He was well rid of her.', \"He certainly didn't want a wife who was fickle as Ann.\", \"If he had married her , he'd have been asking for trouble.\", 'But all of this was rationalization .']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_words = cv.fit_transform(corpus).toarray()\n",
    "vectorized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dan': 7,\n",
       " 'morgan': 19,\n",
       " 'told': 24,\n",
       " 'himself': 16,\n",
       " 'he': 14,\n",
       " 'would': 32,\n",
       " 'forget': 11,\n",
       " 'ann': 1,\n",
       " 'turner': 26,\n",
       " 'was': 28,\n",
       " 'well': 29,\n",
       " 'rid': 22,\n",
       " 'of': 20,\n",
       " 'her': 15,\n",
       " 'certainly': 6,\n",
       " 'didn': 8,\n",
       " 'want': 27,\n",
       " 'wife': 31,\n",
       " 'who': 30,\n",
       " 'fickle': 9,\n",
       " 'as': 2,\n",
       " 'if': 17,\n",
       " 'had': 12,\n",
       " 'married': 18,\n",
       " 'have': 13,\n",
       " 'been': 4,\n",
       " 'asking': 3,\n",
       " 'for': 10,\n",
       " 'trouble': 25,\n",
       " 'but': 5,\n",
       " 'all': 0,\n",
       " 'this': 23,\n",
       " 'rationalization': 21}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.28581119 0.         0.         0.         0.\n",
      "  0.         0.3542556  0.         0.         0.         0.3542556\n",
      "  0.         0.         0.19958143 0.         0.3542556  0.\n",
      "  0.         0.3542556  0.         0.         0.         0.\n",
      "  0.3542556  0.         0.3542556  0.         0.         0.\n",
      "  0.         0.         0.3542556 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27933573 0.40002359 0.         0.\n",
      "  0.         0.         0.40002359 0.         0.49581892 0.\n",
      "  0.         0.         0.         0.         0.33205571 0.49581892\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.27809183 0.34468766 0.         0.         0.\n",
      "  0.34468766 0.         0.34468766 0.34468766 0.         0.\n",
      "  0.         0.         0.19419101 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34468766 0.23084134 0.\n",
      "  0.34468766 0.34468766 0.        ]\n",
      " [0.         0.         0.         0.31749207 0.31749207 0.\n",
      "  0.         0.         0.         0.         0.31749207 0.\n",
      "  0.31749207 0.31749207 0.35773898 0.2561506  0.         0.31749207\n",
      "  0.31749207 0.         0.         0.         0.         0.\n",
      "  0.         0.31749207 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4428322  0.         0.         0.         0.         0.4428322\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35727423 0.4428322  0.         0.4428322\n",
      "  0.         0.         0.         0.         0.29656989 0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "vd = tf.fit_transform(corpus).toarray()\n",
    "print(vd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(newsgroups.data)\n",
    "Y_names = newsgroups.target_names\n",
    "Y = np.array(newsgroups.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
